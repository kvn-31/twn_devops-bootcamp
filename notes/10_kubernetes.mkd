# 10 - Kubernetes

- real-world: applications with hundreds of containers, especially considering microservices
- -> we need container orchestration tools
- open-source container orchestration tool, developed by google
- helps manage containerized applications in different deployment environments

## Features
- high availability, no downtime
- scalability (scale up and down)
- disaster recovery - backup + restore

## Components
- node
    - virtual/physical machine
- pod
    - smallest unit in kubernetes
    - abstraction over a container -> creates running environment/layer on top of container
    - usually 1 application per pod
    - each pod has its own ip address
    - ephemeral -> can die easily -> new ip address is assigned -> this why services are needed
    - normally no interaction with pod, but with the abstraction layer (Deployment)
- service
    - attached to a pod
    - permanent ip address which can be attached to each pod
    - does not die if the pod dies
    - external service: opens communication from external sources
    - internal service: for database f.e.
    - is also a load-balancer
- ingress
    - does the forwarding from a request (to a domain f.e.) to the service (ip)
- config map
    - pods communicate with each other using services (f.e. mongo-db-service), but db url is defined in the application properties/config file/env variable -> what if the name of db service or db url needs to be changed?
    - to solve this problem and avoid a rebuild of the application we use ConfigMap
    - external config of application
    - connected to pod
    - if name of service/endpoint is changed we just change it in config map -> no rebuild is needed
    - used inside the pod with env variables or properties file
    - no confidential data (username pw)
- secret
    - is like ConfigMap, but used for secret data such as credentials
    - important: it is not secure out of the box
    - also connected to pod
    - used inside the pod with env variables or properties file
- Volumes
    - if pod restarts, data would be gone
    - to avoid this we use Volumes
    - attaches a physical storage to a pod
    - storage can be on local machine/remote/outside the cluster
    - kubernetes does not manage data persistance!
- Deployment
    - kubernetes uses distributed systems
    - -> instead of relying on one pod, everything is replicated -> if one pod fails the prod app is not down
    - to do this: blueprint of pod is defined and specified how many replicas are needed
    - this blueprint is called Deployment
    - is an abstraction on top of pods
    - container -> pods -> deployment
- Statefulset
    - Databases cannot be replicated because it has state (risk to have data inconsistencies)
    - for all stateful apps (mysql, psql, ..)
    - makes sure that db reads & writes are synchronized
    - DBs are often hosted outside of K8s cluster
- DaemonSet
    - calculates how many replicas are needed based on existing nodes
    - deploys just one replica per node
    - adds pods if nodes are added
    - removes pods (garbage collected) if nodes are removed
    - no need to define replica count
    - automatically scales up & down by number of nodes

## Architecture

- each node has multiple pods on it (sometimes also called worker nodes)
- to manage: 3 processes are installed
    - container runtime
    - kubelet: interacts with container and node; responsible to actually start the pod with container inside
    - kube proxy: forwards the requests from services to pods
- different container runtimes:
    - docker
    - containerd (most used, more lightweight)
    - cri-o
- docker can run on any container runtime

### Control Plane Nodes
in order to interact with a cluster (schedule pods, monitor, re-start, join new node, ..) control plane nodes are used

4 processes run on every control plane node

- Api server: cluster gateway -> main entrypoint; validates requests and forwards; can be accessed via ui, api or cli (kubectl)
- Scheduler: API server receives request to schedule a new node and sends it to scheduler; has intelligent logic where to put the node; Kubelet actually starts the pod on the node
- Controller Manager: detects state changes (f.e. pod dies), tries to recover state; Controller Manager -> Scheduler -> Kubelet
- etcd: Key-Value store of cluster; cluster's brain; every change in the cluster is updated in the etcd state; must be stored reliable and replicated; no application data stored here

## Example cluster setup

- 2 control plane nodes (more important, but need less resources)
- 3 worker nodes
- can be extended with growing application complexity / demand

## Local Setup Minikube & kubectl

### Minikube
a whole cluster cannot be tested on local machine because of potential lack of memory/processing power -> we use minikube

- is a one node cluster where control plane and worker node processes run on one node/machine
- must start either as container or VM
- the preferred driver is Docker
    - means minikube runs as docker container
    - but inside minikube the applications are also run using docker
- setup: https://minikube.sigs.k8s.io/docs/start/ -> if docker is not installed, have look at drivers page
- start: minikube start --driver docker

#### Minikube commands
```
minikube start
minikube addons enable NAME
minikube dashboard #opens the dashboard in a new window
```

### Assign external service a public ip address with minikube

minikube service NAME

- creates a tunnel and uses the local ip address (no real public ip address)

## Kubectl
used to interact with a cloud or Minikube cluster

- cli tool for K8s cluster
- used to talk to the api server

### Kubectl commands
```
# create a deployment, get latest image from dockerhub
kubectl create deployment NAME --image=IMAGE
kubectl create X

# get all elements of type
kubectl get all
kubectl get node
kubectl get deployment
kubectl get replicaset
kubectl get namespace
kubectl get pod -o wide # with more details such as ip address (handy to see if service is referencing the right pod)
kubectl get deployment nginx-deployment -o yaml > nginx-deployment-result.yaml # save to file with auto-generated status section

# update
kubectl edit deployment NAME

# delete deployment
kubectl delete deployment NAME -> cascades, so that also the replicaset and pod are deleted
kubectl delete -f nginx-deployment.yaml #by file

# logs & information (status changes)
kubectl logs NAME #log to console
kubectl describe pod NAME #info about pod


# interactive terminal inside pod
kubectl exec -it NAME -- bin/bash
```

## Layers of Abstraction

- Deployment manages a ..
- Recplicaset manages a ..
- Pod is an abstraction a ..
- Container

## Configuration File
instead of writing a lot of cli commands to create deployments, we use configuration files which are executed using apply

```
kubectl apply -f FILE
```
Can be used to create or even to update

## YAML Kubernetes Configuration File
on top has apiversion & kind

consists of 3 parts

- metadata (f.e. name)
- spec: for every part of configuration; attributes specific to the kind we create
- status: automatically generated and added by Kubernetes -> will automatically compare desired vs actual state (basis of self-healing); updated continously; status is coming from etcd
    - to access the status
kubectl get deployment nginx-deployment -o yaml

- stored with application code

### Template
bluetprint to a pod: has own metadata -> config applies to pod

### Labels & Selectors
- Metadata contains the label
- spec part contains the selectors
- Pods get a Label
- tell the deployment to match all the labels to create the connection -> deployment knows which pods belong to it
- service can be connected using the selector

service.yaml
```
spec:
    selector:
        app: nginx
```

deployment.yaml
```
metadata:
    labels:
        app: nginx
spec:
    selector:
        matchLabels:
            app: nginx
    template:
        metadata:
            labels:
                app: nginx
```

### Ports
- Service has a port where it is accessible at
- service needs to know to which pod at which port it needs to forward request

service.yaml
```
spec:
  selector:
    app: nginx
  ports:
    - protocol: tcp
      port: 80   # Port number where the service will be accessible within the cluster
      targetPort: 8080  # Port number on which your application is listening
```
deployment.yaml
```
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.25
        ports:
        - containerPort: 8080
```

### Simple setup using service & deployment

```sh
kubectl apply -f nginx-deployment.yaml
kubectl apply -f nginx-service.yaml

# are they correctly connected?
kubectl describe service nginx-service
# output shows the correct selector, target port and correct endpoint ip -> it must be ip addresses of the pods
# how to find ip address of pod?
kubectl get pod -o wide
```

### Demo Project
- Mongo-db & mongo-express
- Simple web application with its database
- flow: client -> external service (mongo express) -> mongo-express pod -> internal service (mongo db) -> mongo db pod
- order of creation matter -> we need secrets before we reference them in the deployment

### Secrets
secret.yaml
```yaml
data:
  mongo-root-username: dXNlcm5hbWU= #base64 encoded, but not encrypted -> not safe!
```
This can be referenced, AFTER it was created in deployment.yaml

```yaml
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          valueFrom:
            secretKeyRef:
              name: mongodb-secret
              key: mongo-root-username
```

### ConfigMap
- external configuration
- centralized
- other components can access it
- as with secret: order of execution matters!
```yaml
          - name: ME_CONFIG_MONGODB_SERVER
            valueFrom:
                configMapKeyRef:
                    name: mongodb-configmap
                    key: mongo-database-url

```

### Service (intro)
- either internal or external
- clusterIp = internal service is the default; only gives internal ip address
- LoadBalancer = external service; also gives external ip address
```yaml
apiVersion: v1
kind: Service
metadata:
  name: mongo-express-service
spec:
  selector:
    app: mongo-express # connects to the pods with the label
  type: LoadBalancer # <- means that the service will be accessible from outside the cluster = external service
  ports:
    - protocol: TCP
      port: 8081
      targetPort: 8081
      nodePort: 30000 # <- this is the port that will be used to access the service from outside the cluster
```

## K8s Namespaces (Detail)

- resources can be organised in namespaces
- a cluster can have multiple namespaces
    - it is like a virtual cluster inside a cluster
- out of box namespaces:
    - kube-system: do not edit/create anything here -> for system processes
    - kube-public: publicly accessible data, configmap with cluster information
    - kube-node-lease: heartbeats of nodes, avialability of nodes
    - default: used if no new namespace is created
- kubectl create namespace NAME
- better: use configuration file

### When is a namespace needed?
if only default namespace is used -> filled quickly; no overview

- for better overview: group recouces in namespaces (f.e. database, monitoring, elastic stack, nginx/ingress)
- to avoid conflicts: if multiple teams work on the same application it can lead to conflicts (if resource has same name, but different config
- for environments in the same cluster (staging, development, ..) and now the namespaces can share the elastic stack namespace for example
- resource sharing: for different versions in production -> might need the same shared resources
- to limit access to resources on namespaces -> each team has own isolated env; also resource quota can be divided between teams

### Characteristics of Namespaces
- most resources of another namespace cannot be accessed (f.e. configmap/secret)
- services can be shared between namespaces (database_url: NAMESPACE.SERVICE)
- volume & node live globally in the cluster, cannot live in a namespace
- by default components are created in default namespace

### Apply Namespaces
- kubectl apply -f FILE --namespace=my-namespace
- directly in file in metadata <- preferred
```
        metadata:
            name: NAME
            namespace: NAMESPACE
```

- change the default/active namespace
- kubectl config set-context --current --namespace=my-namespace
    - after using this, the namespace for current context is set, meaning all subsequent commands will use it

#### Kubens
- even more convenvient tool
- kubens is a tool to switch between Kubernetes namespaces (and configure them for kubectl) easily
- install kubectx, automatically installs kubens

```sh
kubens #see list of namespaces
kubens my-namespace #switch to namespace
```


## Services (Detail)
- pods are ephemeral, are destroyed frequently -> in order to have a stable ip address services are needed
- also provides load balancing

### Types of services

Cluster Ip

- default
- = internal service -> only accessible within the cluster
- is accessible at a certain ip address & port
- is the entrypoint after ingress
- service forwards the request to one of the pods
    - which pod(s)? -> by the selector it matches all the replicas
    - which port? -> defined in targetPort attribute -> has to match the port where the container inside the pod is listening at
- randomly selects to which pods the request is forwarded (load balancer)
- multi port services
    - inside the pod, another container is running (f.e. for monitoring)
    - one service now is responsible for different endpoints (mongodb application and mongodb-exporter f.e.)
    - the service now also needs to have multiple ports open, one for each container it forwards the request to
```yaml
  ports:
    - name: mongodb
      protocol: TCP
      port: 27017
      targetPort: 27017
    - name: mongodb-exporter
      protocol: TCP
      port: 9216
      targetPort: 9216
```

Headless service

- if client/pod want to talk to a specific pod
- not randomly selected
- use case: stateful applications (database) -> pod replicas are not identical; for example mysql + replica -> only the main instance is allowed to write to db; replicas are syncing and only for reading data
- kubernetes provides dns lookup service to return single ip address (ClusterIp); if clusterIp is set to none it returns the pod ip address instead
- is used alongside the ClusterIp service, for specific operations
- headless service has no load balancing, it is just used to talk to a specific pod

NodePort

- external service
- external traffic accessible on a fixed port for each worker node
- instead of using an ingress, the browser request will come directly to worker node
- port exposed=nodePort value (has a fixed range)
- clusterip service to which nodeport service will route is automatically created
- not secure! -> only to test quickly

LoadBalancer
- external service
- becomes accessible externally through cloud providers load balancer
- nodePort and clusterIp services are created automatically
- only accessible through the load balancer


LoadBalancer is an extension of the NodePort service which is an extension of the ClusterIp service

## Ingress

- exposes http route to outside the cluster
- instead of an external service, ingress can be used
- domains can be assigned
- An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting
```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
spec:
  rules:
  - host: "foo.bar.com" #valid domain address
    http: #does not mean it is not https
      paths:
      - pathType: Prefix
        path: "/" # -> foo.bar.com/ 
        backend:
          service:
            name: my-internal-service
            port:
              number: 80 #internal service port
```

- forwards request from the host to the internal service

### Ingress Controller
- evaluates the rules
- manages redirects
- entrypoint to cluster
- needs to be installed -> multiple implementations (even 3rd party)

### Load Balancer
there are different ways to connect to the ingress

- ready-to-use hosting load balancer (f.e. aws): easy setup, forwards progress to ingress
- bare metal
    - some entrypoint needs to be configured
    - can be inside the cluster or outside as separate server
    - for example an external proxy: separate server -> no server in k8s cluster is accessible from outside, public ip address, entrypoint to cluster

### Ingress Controller in Minikube
- minikube addons enable ingress: enables k8s nginx implementation of ingress controller
- minikube acts differently that the "real" web.
- as we obviously do not have the domain "dashboard.com" registered, we will add it to the /etc/hosts file -> 127.0.0.1  dashboard.com
- run minikube tunnel (only needed for local development)



